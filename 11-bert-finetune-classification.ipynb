{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftsaWyMWgML8"
   },
   "source": [
    "# BERT Sentiment Classification (details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSx16UvigML9"
   },
   "source": [
    "Contact: christian.winkler@datanizing.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpboHpI6gML-"
   },
   "source": [
    "Original parts from https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HC8n_sLsgML_"
   },
   "source": [
    "# Load PyTorch and determine GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XW9E06dqgMMA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU %s\" % torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU :-(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOC-fiJPgMMF"
   },
   "source": [
    "# Read labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4pMhLJZgMMG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"10000_All_Beauty.json.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0hrvTB7gMMJ"
   },
   "outputs": [],
   "source": [
    "# Convert labels to integers, torch only works with integers\n",
    "df[\"sentiment\"] = 0\n",
    "df.loc[df[\"rating\"] == 5, \"sentiment\"] = 1\n",
    "df.sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbmOtHSggMMN"
   },
   "outputs": [],
   "source": [
    "# convert to arrays\n",
    "text = df[\"text\"].values\n",
    "labels = df[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e0aURDPgMMQ"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyWJqjINgMMR"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# alternative mode, change max_length to 512 below then\n",
    "# model_name = \"bert-base-uncased\"\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqjhF_asgMMU"
   },
   "outputs": [],
   "source": [
    "# Get maximum length of tokens\n",
    "max_len = max([len(tokenizer.encode(t, add_special_tokens=True)) for t in text])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPZ7oro0gMMZ"
   },
   "outputs": [],
   "source": [
    "# now tokenize everything, get also input_ids and attention_masks\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for t in text:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        t,\n",
    "                        add_special_tokens = True,    # add '[CLS]' and '[SEP]'\n",
    "                        max_length = 1024,\n",
    "                        truncation = True,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,  # create attention masks\n",
    "                        return_tensors = 'pt',         # pytorch tensors as result\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# convert python lists to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[0].numpy()[0:len(tokenizer.tokenize(text[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5URLCp4gMMc"
   },
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xQ1_RXTgMMd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# only work with the tensors starting now\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# use a 3:1 split for training and test\n",
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# set as many random seeds as possible (never enough)\n",
    "torch.manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(train_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcxNtbSEgMMg"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# adjust the batch size accordingly\n",
    "batch_size = 32\n",
    "\n",
    "# use a DataLoader for both datasets (could also use RandomSampler)\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alypLuKZgMMi"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6WF1KWmgMMj"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# model and tokenizer must match\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, device_map=\"auto\",\n",
    "    num_labels = 2, # only positive and negative sentiments\n",
    "    # torch_dtype=torch.bfloat16, # flash attention only works with [b]float16, but results suffer\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False # we don't (yet) need embedings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMBozX1PgMMl"
   },
   "outputs": [],
   "source": [
    "# choose optimizer, AdamW is standard (could also use the pytorch version and avoid the warnung)\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMLp7uE5gMMo"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# four epochs, just a guess\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2n-AetfgMMr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HEOE0WYgMMu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "# initialize again all RNGs\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# save statistics information\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in trange(epochs, desc=\"Epoche\"):\n",
    "    # accumulated loss for the current epoch\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # put model into training mode (save gradients)\n",
    "    model.train()\n",
    "\n",
    "    # train per batch\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        # unpack data and transform to device format\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # clear gradient\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # forward pass (predict data)\n",
    "        res = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # calculate and accumulate loss\n",
    "        total_train_loss += res.loss.item()\n",
    "\n",
    "        # backward propagation to calculate gradient\n",
    "        res.loss.backward()\n",
    "\n",
    "        # cut to acoid exploding gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # optimizer weights and learning rate\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "      \n",
    "\n",
    "    # put model into evaluation mode (don't save gradients)\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # validate a single epoch\n",
    "    for batch in tqdm(validation_dataloader, desc=\"Validation\"):\n",
    "        # unpack validation data\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # noch backward propagation, don't calculate gradient\n",
    "        with torch.no_grad():        \n",
    "            # make prediction\n",
    "            res = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "            \n",
    "        # accumulate eval loss\n",
    "        total_eval_loss += res.loss.item()\n",
    "\n",
    "        # convert data to cpu format (calculate accuracy)\n",
    "        logits = res.logits.detach().cpu().float().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Accuracy for verification\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    tqdm.write(\"Accuracy: %f\" % avg_val_accuracy)\n",
    "\n",
    "    # Loss for all batches\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    tqdm.write(\"Validation loss %f\" % avg_val_loss)\n",
    "\n",
    "    # save statistics for plotting later\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Validierung Loss': avg_val_loss,\n",
    "            'Accuracy': avg_val_accuracy\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRtp5jhhgMMy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats).set_index(\"epoch\")\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpGePApMgMM3"
   },
   "outputs": [],
   "source": [
    "df_stats.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BERT classification heise ticker",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "finetuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
